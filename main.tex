\pdfminorversion=4
\documentclass[aspectratio=169]{beamer}

\mode<presentation>
{
  \usetheme{default}
  \usecolortheme{default}
  \usefonttheme{default}
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]  % or "page number"
  \setbeamercolor{frametitle}{fg=white}
  \setbeamercolor{footline}{fg=black}
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{tikz}
\usepackage{courier}
\usepackage{array}
\usepackage{bold-extra}
\usepackage{minted}
\usepackage[thicklines]{cancel}
\usepackage{fancyvrb}

\xdefinecolor{dianablue}{rgb}{0.18,0.24,0.31}
\xdefinecolor{darkblue}{rgb}{0.1,0.1,0.7}
\xdefinecolor{darkgreen}{rgb}{0,0.5,0}
\xdefinecolor{darkgrey}{rgb}{0.35,0.35,0.35}
\xdefinecolor{darkorange}{rgb}{0.8,0.5,0}
\xdefinecolor{darkred}{rgb}{0.7,0,0}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\title[2018-08-21-cmsblueprint-querysystem]{Query-based analysis: how to get there?}
\author{Jim Pivarski}
\institute{Princeton University -- DIANA-HEP}
\date{August 21, 2018}

\usetikzlibrary{shapes.callouts}

\begin{document}

\logo{\pgfputat{\pgfxy(0.11, 7.4)}{\pgfbox[right,base]{\tikz{\filldraw[fill=dianablue, draw=none] (0 cm, 0 cm) rectangle (50 cm, 1 cm);}\mbox{\hspace{-8 cm}\includegraphics[height=1 cm]{princeton-logo-long.png}\includegraphics[height=1 cm]{diana-hep-logo-long.png}}}}}

\begin{frame}
  \titlepage
\end{frame}

\logo{\pgfputat{\pgfxy(0.11, 7.4)}{\pgfbox[right,base]{\tikz{\filldraw[fill=dianablue, draw=none] (0 cm, 0 cm) rectangle (50 cm, 1 cm);}\mbox{\hspace{-8 cm}\includegraphics[height=1 cm]{princeton-logo.png}\includegraphics[height=1 cm]{diana-hep-logo.png}}}}}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

% START START START START START START START START START START START START START

\begin{frame}{Query-based analysis}
\large
\vspace{0.5 cm}
\textcolor{darkorange}{\bf Different performance characteristics $\to$ sociological consequences}

\vspace{1 cm}
\begin{columns}
\column{0.3\linewidth}
\underline{\Large Batch job}

\vspace{0.2 cm}
Long time to compute one complex function
\column{0.3\linewidth}
\underline{\Large Query}

\vspace{0.2 cm}
Short time to compute each simple function
\end{columns}

\vspace{1 cm}
\begin{uncoverenv}<2->
\begin{tikzpicture}[>=latex]
\node[rectangle callout, draw, inner sep=5 pt, callout relative pointer={(200:1 cm)}, text width=0.47\linewidth] {\large Optimize execution of well-defined procedures, like central reconstruction};
\end{tikzpicture} \hfill \begin{tikzpicture}[>=latex]
\node[rectangle callout, draw, inner sep=5 pt, callout relative pointer={(-200:-1 cm)}, text width=0.45\linewidth] {\large Make and correct mistakes, discover blind alleys early, find surprises};
\end{tikzpicture}
\end{uncoverenv}
\end{frame}

\begin{frame}{Different ``performance characteristics''}
\large
\vspace{0.5 cm}
Not promising a miracle--- more computation in less time--- but scaling that fits many short jobs (``queries'') instead of few big jobs.

\vspace{0.5 cm}
\uncover<2->{\textcolor{darkblue}{\underline{What needs to change:}}}

\vspace{0.2 cm}
\begin{itemize}
\item<3-> Startup time must be below a human timescale, like 100~ms. \textcolor{gray}{Workers must be ready and waiting, not launched on demand.}
\item<4-> Input data (which will always be large, like TB/query) must be cached on the workers. \textcolor{gray}{And work should be performed by the worker who already has the relevant fragment in cache! Dispatch by cache-locality.}
\item<5-> Moving and transforming data from cache to the user function must not dominate over the user function. \textcolor{gray}{(This is what I've been focusing on.)}
\item<6-> Output data must be small, like MB/query--- GB/query at most.
\end{itemize}
\end{frame}

\begin{frame}{Query services in the wild}
\hfill \includegraphics[height=1.5 cm]{google-bigquery-logo.png}

\vspace{-1.25 cm}
Google BigQuery is a public-facing SQL processor that runs my \\ 2~TB queries in 30~seconds.

\vspace{0.5 cm}
\hfill \uncover<2->{\includegraphics[height=1 cm]{binder-logo.pdf}}

\vspace{-1.1 cm}
\uncover<2->{Binder is a free service that runs any Jupyter Notebook in a \\ GitHub repository. \textcolor{gray}{(Not quite ``query'' due to notebook state.)}}

\vspace{0.25 cm}
\begin{uncoverenv}<3->
\textcolor{gray}{\small
Search on GitHub: {\tt\scriptsize binder in:path path:/binder} yields \textcolor{black}{9,754} Binder setup files. \\
Search on GitHub: {\tt\scriptsize https://mybinder.org/badge.svg language:Markdown} yields \textcolor{black}{3,835} badges.}
\end{uncoverenv}

\begin{uncoverenv}<4->
\begin{columns}
\column{0.37\linewidth}
\small
$\sim$200 notebooks running at a time, served by 5 machines worldwide

\vspace{0.25 cm}
\scriptsize Tim Head: ``The trick is to oversubscribe the CPU massively; turns out humans take a long time to read what a notebook computed for them.''

\normalsize
\vspace{0.25 cm}
\centering 1 user $\neq$ 1 CPU!

\column{0.55\linewidth}
\includegraphics[height=3 cm]{binder-notebooks-running.png}\mbox{ }\includegraphics[height=3 cm]{binder-utilization.png}
\end{columns}
\end{uncoverenv}
\end{frame}

\begin{frame}{Query system for HEP}
\vspace{0.35 cm}
Our problem is structured more like Google BigQuery than Binder:

\vspace{0.15 cm}
\begin{itemize}
\item no notebook state; each query is a pure function
\item large source datasets, embarrassingly parallel, distributed across workers
\item big outputs stay on the server as new sources
\item small outputs download to users
\end{itemize}

\vspace{0.15 cm}
\begin{center}
\includegraphics[width=0.6\linewidth]{basic-block-diagram.pdf}
\end{center}
\end{frame}

\begin{frame}{More considerations}
\vspace{0.25 cm}
\hfill \mbox{\includegraphics[height=3 cm]{basic-block-diagram.pdf} \hspace{-0.75 cm}}

\vspace{-3 cm}
\begin{itemize}\setlength{\itemsep}{0.5 cm}
\item<1-> Sources are {\it shared} across users, some more \\ popular (likely in cache) than others.

\vspace{0.1 cm}
\begin{itemize}
\item \underline{955 CMS NanoAOD branches}
\item \textcolor{white}{0}70 are 4-vector components
\item 127 are ``id'' variables
\item 153 are isolation variables
\item 601 are HLT booleans (may be bitmaps)
\end{itemize}

\vspace{0.1 cm}
\hspace{-0.5 cm}Columns (branches) should be the granular unit of reading and caching on workers.

\item<2-> Some variable transformations, like jet energy corrections, could be useful to many users as new sources. Data should be sharable with columnar granularity (e.g.\ use centrally produced isolation with user-corrected energy).

\item<3-> Reduced outputs (histograms and tuples for maximum likelihood fitting) are specialized--- unlikely to be useful to multiple users--- less worth caching.
\end{itemize}
\end{frame}

\begin{frame}{Ingredients}
\vspace{0.2 cm}
\begin{block}{\underline{Columnar data handling}}
\textcolor{darkorange}{\bf uproot} to read data by column from ROOT files

\textcolor{darkorange}{\bf awkward-array} to manipulate jagged data structures as columns \mbox{(Numpy-like interface)\hspace{-1 cm}}
\end{block}

\begin{block}{\underline{Data delivery}}
\textcolor{darkorange}{\bf blosc} for on-the-fly compression?

\textcolor{darkorange}{\bf cachetools} for LRU? \textcolor{darkorange}{\bf diskcache} for spilling to disk?

Object-store like \textcolor{darkorange}{\bf Ceph}? Database like \textcolor{darkorange}{\bf CouchBase}?
\end{block}

\begin{block}{\underline{Cache-local dispatch}}
Build a non-standard queue in \textcolor{darkorange}{\bf Zookeeper}?

Any other option to post a task pool and {\it pull} work to appropriate workers?
\end{block}

\begin{block}{\underline{Worker implementation}}
Simple HTTP modules?

Microframework like \textcolor{darkorange}{\bf Flask}? Specialized framework like \textcolor{darkorange}{\bf Dask} or \textcolor{darkorange}{\bf Spark}? \textcolor{gray}{(push only)}
\end{block}
\end{frame}



\end{document}
